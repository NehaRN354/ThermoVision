AI-Powered Hazard Awareness for the Visually Impaired

Overview

ThermoVision is an AI-driven real-time hazard awareness system designed to assist visually impaired individuals by detecting dangerous environmental elements and converting them into clear audio safety feedback.

Instead of relying heavily on specialized hardware, the project emphasizes intelligent computer vision, lightweight deep learning models, and risk-based decision making to enable safe and independent navigation.

Core Capabilities

ğŸ”¥ Fire Detection

CNN-based classification

Color pattern analysis

Flicker recognition for real-time flame confirmation


ğŸ§ Human Detection & Proximity

YOLO object detection

Distance estimation for spatial awareness


ğŸš— Vehicle & Hot Appliance Detection

MobileNet lightweight vision model

Optimized for real-time embedded inference


âš ï¸ Risk Assessment Engine

Combines all detection outputs into three intuitive safety states:

Safe â€“ No immediate hazard

Caution â€“ Potential nearby risk

Danger â€“ Immediate threat detected


ğŸ”Š Audio Feedback

Converts risk level into spoken alerts

Enables hands-free situational awareness


ğŸŒ™ Low-Light Awareness

Detects poor illumination conditions

Maintains reliability beyond normal visibility


AI & Software Stack

Deep Learning: CNN, YOLO, MobileNet

Computer Vision: OpenCV, preprocessing, feature extraction

Language: Python

Inference: Real-time lightweight model execution

Designed for efficiency, scalability, and wearable deployment.


System Workflow
Scene Input â†’ Parallel AI Detection
           â†’ Fire | Human + Distance | Vehicle/Hot Object
           â†’ Risk Assessment Engine
           â†’ Audio Safety Feedback
           â†’ Low-Light Handling


Applications

Assistive navigation for visually impaired users

Indoor fire and hazard awareness

Smart wearable safety systems

Real-time environmental monitoring

Research & Innovation Value


ThermoVision shifts assistive technology from passive sensing to active environmental intelligence.

By combining:

real-time perception

multimodal hazard detection

interpretable risk feedback

the system moves toward a future where safe and independent mobility for the visually impaired.
